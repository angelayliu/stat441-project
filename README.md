# stat441-project
## Task Description
The final project for this course was to strive for the highest prediction score in a Kaggle competition against our classmates. Given a dataset of survey results, where each observation was a survey response from one person in Europe, our task was to predict "How is religion important in one's life" on a 4-point scale ranging from most to least important (labelled 1 to 4, respectively), or if there was no answer (labelled -1). The prediction score was calculated using a multiclass log-loss of our model's predictions on an unlabelled dataset.
## Approach
### EDA, Data Preprocessing, and Feature Selection
(See [Data Preprocessing and Cleaning.ipynb](https://github.com/angelayliu/stat441-project/blob/main/Data%20Preprocessing%20and%20Cleaning.ipynb))

By studying the given codebook, which was detailed with complete identification of all features and context to the range of values they take on, then confirming with the dataset, there are 3 continuous features of the total 438, the rest being categorical features. Of those categorical features, roughly half were nominal, the other half ordinal. The decision was made to exclude data visualization due to the vast number of features. There were 8 string-typed categorical features all with corresponding features that numerically coded the same information. Therefore, the decision was made to one-hot-encode the string-typed features and remove their numerically coded counterparts to avoid a high correlation between features, which could negatively impact many learning algorithms. Since tree-based algorithms handle categorical features without one-hot-encoding just as well as continuous ones, the remaining categorical features were not one-hot-encoded. Also, tree-based algorithms naturally perform feature selection as the model is built, so no additional steps were taken to manually select features. The last preprocessing step was to change all -1 labels to 0 to ensure learning algorithms ran correctly. All the steps above resulted in improving cross-validated training prediction scores, which extends to prediction scores on testing data.
### Modelling
(See [stack_default.ipnb](https://github.com/angelayliu/stat441-project/blob/main/Stacking/stack_default.ipynb))

After conducting trial-and-error with many classification learning algorithms, the approach which yielded the best prediction score by a large margin and in an efficient time frame was to stack an untuned multinomial gradient boosting model (implemented using LightGBM) with an untuned XGBoost model. The meta-model used for stacking was another XGBoost model, which was tuned to achieve the best training score.
### Inferencing
Unlike other learning models where features are more interpretable, tree-based methods operate in a black-box manner. An alternative way to explore how important the features are for tree-based algorithms is to calculate variable influence and create an influence bar chart -- as seen in [LightGBM.ipynb](https://github.com/angelayliu/stat441-project/blob/main/Gradient%20Boosting/LightGBM.ipynb). Two influence bar charts were created, one for each of our base learners. Across the two graphs, it was evident that the most influential features fell into the following categories: religion (frequency of activities, importance, confidence), geographical region, education, income, and personal beliefs.
## Results
Achieved a grade of 90% with a private Kaggle score of 0.84697, which ranked amongst the top 10% of the class.
## Further Extensions
Using CatBoost to implement gradient boosting could yield better prediction scores as this framework is more suitable for categorical features, which close to 99% of the features were in this dataset. Another extension is to add a successfully tuned or semi-tuned neural net to the stack, as well as experiment with the weights of each model.
